# Abstract

文章提出了一种简单的 seq2seq 方法，用于科学文本中复杂层次信息的联合 NER 和 RE。该方法利用了预先训练好的大型语言模型（LLM），GPT-3，该模型在大约500对提示（输入）和完成（输出）上进行了微调。信息可以从单个句子中提取，也可以从摘要/段落中的不同句子中提取，输出可以以简单的英语句子或更有结构的格式返回，如 JSON 对象的列表。我们证明，以这种方式训练的LLM能够准确地提取材料化学中三个代表性任务的复杂科学知识的有用记录：将掺杂物与它们的宿主材料联系起来，对金属有机框架进行编目，以及一般化学/相/形态/应用信息的提取。这种方法代表了一种简单、易得和高度灵活的途径，以获得从非结构化文本中提取的结构化知识的大型数据库。

现有的 RE 模型并不是被设计去提取或保存 任意数量的named entities之间高度复杂，错综相关和层级关系。

<!--在这项工作中，我们研究了一种简单易行的复杂信息提取方法，其中对大型语言模型进行了微调，以同时进行文档级NER和RE。该方法简单但能够灵活处理复杂的相互关系（包括信息存在于层次结构中或作为多个项目的列表的情况），而不需要枚举所有可能的 n 元组关系或初步 NER。我们对大型语言模型 GPT-3 进行微调，以接受文本段落（例如，研究论文摘要）并编写提示中包含的知识的格式精确的“Abstract”。--> 

<!--此完成可以格式化为英语句子或更结构化的模式，例如 JSON 文档列表。要使用此方法，只需定义所需的输出结构----例如，具有一组预定义键的 JSON 对象列表----并使用此格式注释～ 100 - 500 个文本段落。然后 GPT-3 在这些示例上进行微调，生成的模型能够准确地从文本中提取所需信息，并以相同的结构化表示形式输出信息，如图 1 所示-->

训练 GPT-3 以执行 NERRE 任务的一般工作流程 Fig2：先人工标注一部分数据，得到初始训练集后进行训练，得到一个（部分训练过的）模型，然后用这个模型对另一波数据进行标注，这种标注可能有错误，对这种错误进行人工修正，再放入模型进行训练。重复多次，每次模型的性能都会提升。这种循环的结构可以加速标注的速度。

# Methods

## Benchmark tasks and output schema design

### Solid-state impurity doping task

- host：主体晶体、样品或材料类别及其直接上下文中的关键描述符（例如，“ZnO2 纳米粒子”、“LiNbO3”、“half-Heuslers”）
- dopant：是任何少数元素或离子、有意添加的杂质或特定的点缺陷或电荷载流子（“hole-doped”、“S vacancies”）

<!--我们为掺杂任务训练和评估了三个独立的模型。前两个模型，Doping-JSON和Doping-ENG，都旨在只提取掺杂物和宿主实体以及掺杂物和宿主之间的关系。Modifier 和 result 实体被忽略了。如图3所示，这些模型仅在其结构化完成的格式（schemas）上有所不同。Doping-JSON模型使用的是JSON模式，代表了单句中宿主和掺杂物之间的关系图，其中唯一的键识别掺杂物和宿主的字符串。该模型的目的是在微调期间学习这个相对宽松的模式。一个单独的键 "hosts2dopants "根据这些唯一的键来描述配对关系。-->

<!--Doping-ENG 模型将实体关系编码为准自然语言的摘要。这些摘要代表了与JSON模式相同的信息，但它们更接近于模仿GPT-3的自然语言预训练分布。这个模式是根据补充章节中显示的关于参杂物模式的程序化填充的，其中存在的每个兴奋剂条目都是用新行分隔的。例如，第一行详细描述了一个宿主到多个掺杂物的关系，第二行表达了一个没有宿主的掺杂物实体，第三行表达了一个没有掺杂物的宿主实体。-->

DopingExtra-ENG 另外提取doping modifier 和 result 数据

---

### General materials information schema

在我们以前的工作中[3, 4]，我们着重研究了与材料科学特别相关的特定实体类型的NER：材料、应用、结构/相标签、合成方法等。然而，除了简单的 "实体袋 "方法外，我们无法将这些标记的实体联系在一起以记录它们的关系。在这项工作中，我们训练了一个LLM来执行一个 "一般材料信息提取 "的任务，这个任务既能捕捉到实体，又能捕捉到它们之间复杂的相互作用网络。

我们为这项任务设计的模式囊括了关于固体化合物及其应用的重要信息子集。列表中的每个条目是一个自含的JSON文档，与文本中提到的材料一一对应。材料条目是按文本中的出现顺序排列的。每个条目的根以化合物的名称和/或其化学式开始。如果某种材料没有提到名称或化学式，就不会从文本中提取有关该材料的信息。我们也提取材料名称/公式中提到的缩写，尽管在只提到缩写的情况下，我们不会为该化合物创建材料条目。非固体的化合物（离子、液体、溶剂、溶液等）一般不会被提取。在JSON文档中，每个材料的 name、formula 和 acronym 字段都是专门给定的字符串值，而description、structure_or_phase 和 application 字段是由任意数量的字符串组成的列表。我们给这个模型贴上General-JSON的标签，**图 4** 中显示了一个例子。

Descriptions 被定义为关于化合物的加工历史、缺陷、修改或样品的形态的细节。例如，考虑假设的文本 "Pt supported on CeO2 nanoparticles infused with Nb..."。在这种情况下，材料文件中提到 "Pt "的 "descriptions" 条目可能被注释为"['supported on CeO2']"，而文件中提到 "CeO2 "的条目将是"['nanoparticles', 'Nb-doped']"。

结构/相位被定义为直接暗示化合物的晶体结构或对称性的实体。晶体系统如 "立方体 "或 "四方体"，结构名称如 "金红石 "或 "NASICON"，空间群如 "Fd3m "或 " space group No. 93 "都在这个领域中提取。我们还包括任何有关晶体单元的信息，如晶格参数和晶格向量之间的角度。"无定形 "也是一个有效的结构/相标签。

Application 被定义为材料的高级用例或主要属性类别。应用可以代表不同层次的设备级实现。例如，一个电池阴极材料可能有"['锂离子电池'，'阴极']"作为其应用条目。一般来说，应用是按照文本中的顺序提及的，但某些情况除外，如电池材料，在这种情况下，设备的类型一般在电极类型之前提及，而催化剂，其催化的反应一般列在列表中的 "催化剂 "实体之后（例如，"['催化剂'，'柠檬醛的氢化'"）。这个模式的更多细节和注意事项在补充资料中给出。

---

## Comparison benchmarks and evaluation





## Evaluation criteria

由于 Entities 和 Relationships 之间的模糊性和复杂性，我们需要使用多个指标进行评分。在两个层面上评估所有模型的性能：sequence reconstruction 和 information extraction performance。

### sequence reconstruction:

- exact sequence match accuracy, 
- Jaro-Winkler similarity, 
- and parsability

我们只是衡量模型输出一个与真实字符串相似的字符串的能力。

---

### information extraction performance

two methods of scoring entity-relation data

- A triplet $F_1$ 
  - 有多少词语是完全按照它们在源文本提示中出现的方式正确连接在一起的
- and a less-stringent triplet $F_1$
  - 有多少个词组在上下文中是近似正确的

#### Exact word-match basis scoring

对 NER 而言，使用 Exact word-match basis scoring

<!--准确的词匹配基础评分。我们以词为基础对命名的实体关系进行评分，首先将实体E转换为一组由K个空白分隔的词E={w1,w2,w3,...,wk}组成。仅就 NER 而言，比较两个实体Etrue和Etest，我们将两个集合中完全匹配的词的数量算作真阳性（Etrue ∩ Etest），将两个集合之间的数学差异算作假阳性（Etest- Etrue）或假阴性（Etrue- Etest）。例如，如果真实的实体是 "Bi2Te3薄膜"，而预测的实体是 "Bi2Te3薄膜样品"，我们就会记录两个真阳性词的精确匹配（"Bi2Te3"，"薄膜"），一个假阴性（"薄膜"），和一个假阳性（"样品"）。这种方法的唯一例外是对识别材料至关重要的公式型实体，在这种情况下，Etest必须包含所有可解析为化学计量的wi，任何wi∈Etest都被认为是正确的。例如，如果真实的实体是 "Bi2Te3薄膜"，而预测的实体是 "薄膜"，我们就会记录三个错误的否定。因此，任何包含化学成分的公式型实体（掺杂宿主、掺杂掺杂剂、一般公式和MOF mof_formula），如果成分不是完全匹配，则完全不正确。选择这种评价方式是为了避免指标人为地夸大模型的性能。例如，如果我们将一个真实的 "Bi2Te3纳米颗粒 "与一个预测的 "Bi2Se3纳米颗粒 "进行评估，我们通过Jaro-Winkler（0.977）和字符级BLEU-4（0.858）得到非常高的相似度。实际上，这个例子的预测应该被记录为完全不正确--材料的化学成分是错误的-->

现在我们用正确的三联体的数量对实体之间的关系进行字数上的评分。三联体是指通过关系 $r$ 将实体 $E_n$ 的单词 $w^n_j$ 与实体 $E_m$ 的单词 $ w^m_k$ 联系起来的 3-tuples，表示为 $(w^n_j,w^m_k,r)$。一个文本的正确关系总集Ttrue包含许多这样的三联体。一个测试的关系集Ttest是通过计算在两个关系集中发现的三联体的数量（Ttrue∩Ttest）作为真阳性，以及这些关系集之间的差异作为假阳性（Ttest- Ttrue）或假阴性（Ttrue- Ttest）来评估的。如果三联体中的任何一个词属于一个公式型实体（host, dopant, formula, mof_formula），实体三联体也被约束在同样的组成正确性要求下，即如果公式不是精确的字符串匹配，我们将两个实体的所有三联体算作不正确。在确定了正确和不正确的三联体后，每个关系的F1scores被计算为

。。。

#### Manual evaluation

为了更好地解决这些模糊任务的评分问题，我们引入了一个基于领域专家对所提取的信息是否是段落中实际包含的信息的有效表述的调整分数。我们把这个调整后的分数称为 "人工信息提取分数"；它构成了精确性、召回率和F1的基础，在可能有同等或多种方式表示同一概念的情况下，量化了整体信息捕获的质量。构建这个分数是为了更好地估计我们的模型在实际材料信息提取任务中的表现。

我们通过注释者提取的实体，但在模型的输出中没有出现的实体作为假阴性打分，除非存在合理的变化。真阳性的标准如下

# Results

我们首先报告第二节中描述的序列级评估指标的结果，以便让读者了解LLM NERRE模型的输出序列有多大可能是完全正确的、有点正确（通过Jaro-Winkler）或可解析的（即格式良好且符合模型训练的模式）。序列级指标在高层次上代表了模型 "遵循规则 "的可能性。接下来，我们在材料NERRE任务上对模型进行评估，对单个关系和实体进行更细化的评估，并与基于BERT的接近方法和seq2rel进行比较；这些结果将使读者获得对模型性能的更深入的看法。最后，我们使用General-JSON 模型与BatteryBERT进行比较，并考察了训练集大小对 doping 任务性能的影响

# Conclusion

我们提出了一种简单的提示和完成工程方法，用于从使用大型seq2seq语言模型的非结构化科学文本中提取复杂的、分层的和高度特定领域的关系信息。这个方法是高度灵活的，我们希望它可以很容易地适应科学领域内的许多问题。在应用这种方法时，我们发现在材料工程的三个不同的任务上表现出色：固态杂质掺杂、金属有机框架和一般材料关系。这种方法的非技术性意味着没有经过NLP培训的科学家可以利用现有的seq2seq模型，如GPT-3，为高度具体的问题提取大型结构化的关系数据集。由于LLM基本上被视为一个黑盒子，我们预计这种方法可能会被用于GPT-3以外的LLM，包括在不久的将来发布的LLM。我们希望这种方法能使领域专家快速提取关系数据集，以促进科学知识的发展。























