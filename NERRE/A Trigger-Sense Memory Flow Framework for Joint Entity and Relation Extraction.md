[toc]

# Abstract

联合实体和关系抽取框架构建了一个统一的模型来同时进行实体识别和关系抽取，利用了两者之间的依赖关系来缓解管道模型的错误传播问题。目前在联合实体和关系提取方面的努力主要集中在通过参数共享、联合解码或其他特殊技巧(例如，建模为半马尔可夫决策过程，作为多轮阅读理解任务)来增强实体识别和关系提取之间的交互。然而，仍有两个问题有待解决。

首先，大多数方法利用的交互仍然是弱的、单向的，无法对两个任务之间的相互依赖进行建模。其次，大多数方法忽略了关系触发器，这可以帮助解释为什么人类会在句子中提取关系。它们对于关系提取至关重要，但却被忽视了。

为此，我们提出了一个用于联合实体和关系提取的触发感知记忆流框架(TriMF)。我们建立了一个**记忆模块**来记忆在实体识别和关系提取任务中学习到的类别表示。在此基础上，设计了**多级记忆流注意机制**，增强实体识别与关系提取的双向交互。

此外，我们的模型可以在没有任何人工注释的情况下，通过触发传感器模块增强句子中的关系触发信息，从而提高模型性能，使模型预测具有更好的解释性。

实验结果表明，本文提出的框架在SciERC、ACE05、CoNLL04和ADE上的关系F1分别达到52.44%(+3.2%)、66.49%(+$4.9$%)、72.35%(+0.6%)和80.66%(+2.3%)，达到了最先进的结果。

# TRIGGER-SENSE MEMORY FLOW FRAMEWORK

在本节中，我们将介绍用于实体和关系联合抽取的Trigger- sense Memory Flow框架(TriMF)，该框架由5个主要模块组成:

- Memory module, MultiLevel Memory Flow Attention module, Syntactic-Semantic Graph Weighted Fusion module, Trigger Sensor module, and Memory-Aware Classifier module.
- Memory模块、多级Memory流注意模块、句法-语义图加权融合模块、触发传感器模块和Memory感知分类器模块。

TriMF的总体架构如图2所示。我们首先初始化记忆，包括一个实体记忆 $\mathbf{M}^\mathcal{E}\in\mathbb{R}^{n^e\times h_{me}}$ 和一个关系记忆$\mathbf{M}^\mathcal{R}\in\mathbb{R}^{n^r \times h_{mr}}$，其中 $n^e$ 和 $n^r$ 表示实体类别和关系类别的数量，$h_{me}$ 和 $h_{mr}$ 表示实体记忆和关系记忆的插槽大小。

![image-20230531143615354](https://s2.loli.net/2023/05/31/JoGL9tjuc1zhOXQ.png)

我们的模型执行了四级句子编码(子词、词、跨度和跨度对，如图1所示)和两步分类(实体分类和关系分类)。更具体地说，一个句子被BERT编码，得到编码E𝑑= R𝑚×ℎ的子词序列，其中𝑚表示句子中的子词数量，ℎ表示BERT的隐藏状态大小。以MR、ME和E𝑑为基础，在子字级执行第一个内存流注意力。然后我们使用𝑓𝑤将子词序列编码聚合成一个词序列编码E𝑤= R𝑛×ℎ𝑤，其中𝑛表示句子中的单词数量，ℎ𝑤表示词向量的大小。对于𝑓𝑤，我们采用最大池化函数。以MR、ME和E𝑤为基础，在单词级执行二次记忆流注意。然后，将得到的词序列编码输入到句法-语义图加权融合模块中，在词层面融合语义和句法信息;然后，我们通过𝑓𝑠对单词序列编码进行组合得到span序列编码E𝑠= R𝑁×ℎ𝑠，其中𝑁表示句子中的span数量，ℎ𝑠表示span向量的大小。在这里，对于𝑓𝑠，我们采用一种在最大池化的词嵌入上连接跨大小嵌入的方法。我们通过内存感知实体分类器过滤掉被分类为None类别的span。在对感兴趣的范围进行配对后，我们使用触发传感器计算局部上下文表示𝑔𝑙𝑜𝑐𝑎𝑙和全部上下文跨度对特定的触发表示g𝑡𝑟𝑖𝑔𝑔𝑒𝑟。我们把编码的头,尾,𝑔𝑙𝑜𝑐𝑎𝑙和g𝑡𝑟𝑖𝑔𝑔𝑒𝑟获得编码E𝑟∈R𝑀×ℎ𝑟

其中 E𝑟 (𝑖 𝑗) 表示由 𝑖𝑡ℎ 和 𝑗𝑡ℎ 跨度组成的跨度对编码，𝑀 表示候选跨度对的数量，ℎ𝑟 表示跨度对编码的大小。最后，我们将候选跨度对表示输入到记忆感知关系分类器，并预测两个跨度之间的关系类型。在接下来的部分中，我们将详细介绍模型的五个主要模块。

