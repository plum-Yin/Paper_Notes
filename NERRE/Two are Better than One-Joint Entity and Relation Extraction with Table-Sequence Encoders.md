<!--一个典型的方法是首先识别实体的提法，然后在每两个提法之间进行分类以提取关系，形成一个管道。 另一种较新的方法是联合执行这两项任务，这样可以减轻与管道方法相关的错误传播问题，并利用任务之间的互动，从而提高性能。-->

<!--在几种联合方法中，一种流行的想法是将NER和RE作为一个**填表问题**。通常情况下，形成一个二维（2D）表，其中每个条目捕捉到句子中两个单独单词之间的相互作用。 然后，NER被看作是一个序列标签问题，标签沿着表的对角线条目分配。 RE被认为是表内其他条目的标签问题。 这种方法允许使用一个单一的模型来进行NER和RE，使这两个任务之间有潜在的有用的互动。图1展示了一个例子。-->

<!--不幸的是，现有的联合方法存在着局限性。首先，这些方法通常受到**特征混淆**的影响，因为它们对两个任务（NER和RE）使用单一的表示法。因此，为一个任务提取的特征可能与另一个任务的特征重合或冲突，从而混淆了学习模型。第二，这些方法**没有充分利用**表的结构，因为它们通常将其转换为序列，然后使用序列标记的方法来填充表。 然而，二维表中的关键结构信息（例如，图1左下角的4个条目共享相同的标签）在这种转换中可能会丢失。-->

<!--在本文中，我们提出了一种新的方法来解决上述限制。我们的重点是学习两类表征，即序列表征和表格表征，而不是用单一的表征来预测实体和关系，分别用于NER和RE。一方面，这两种独立的表征可以用来捕捉特定的任务信息。另一方面，我们设计了一种机制，使它们能够相互作用，以利用NER和RE任务的内在联系。此外，我们采用的神经网络架构可以更好地捕捉二维表格表示中的结构信息。正如我们将看到的，这种结构信息（特别是表中相邻条目的背景）对于实现更好的性能至关重要。-->

<!--最近BERT的盛行（Devlin et al.,2019）使得各种NLP任务的性能得到了极大的提升。然而，我们认为，以前对BERT的使用，即采用上下文的单词嵌入，并没有完全发挥其潜力。这里的一个重要观察是，由BERT维护的成对的 self-attention 权重带有词与词之间相互作用的知识。我们的模型可以有效地利用这些知识，这有助于更好地学习表征。据我们所知，这是第一个使用BERT的注意力权重来学习表格表征的工作。-->

我们把我们的贡献总结如下：

- 我们建议学习两个独立的编码器：一个表格编码器和一个序列编码器。 它们相互作用，并能为NER和RE任务捕捉特定的信息；
- 我们建议使用多维 RNN 来更好地利用表的结构信息；
- 我们有效地利用了BERT的注意力权重中的词与词之间的信息，从而进一步提高了性能。

Related works 都没有使用预先训练好的注意力权重，而注意力权重传达了词与词之间的丰富关系信息。我们相信这对学习更好的RE表征是很有用的。

# Model

该模型由两种相互连接的编码器组成，一种是用于表格表示的表格编码器，一种是用于序列表示的序列编码器，如图2所示。我们把它们统称为 table-sequence 编码器。图3展示了这两个编码器的每一层的细节，以及它们如何相互作用。在每一层中，表格编码器使用序列表示法来构建表格表示法；然后序列编码器使用表格表示法来确定序列表示法的上下文。通过多层，我们逐步提高这两种表示的质量。

## Table Encoder

上面的插图描述了一个单向的RNN，对应于图4(a)。直观地说，我们希望该网络能在所有方向上都能访问周围的环境。然而，这不可能由一个单一的RNN完成。对于一维序列建模的情况，这个问题可以通过引入双向的RNN来解决。Graves等人（2007）讨论了四向RNNs，从四个方向访问上下文，用于二维数据建模。因此，与2D-RNN类似，我们也需要考虑四个方向的RNN。我们在图4中把它们可视化。

根据经验，我们发现在图4中只考虑(a)和(c)情况的设置所取得的性能并不比完全考虑四个情况差。因此，为了减少计算量，我们把这种设置作为默认值。最后的表格表示是两个RNN的隐藏状态的串联：

## Sequence Encoder

序列编码器用于学习序列表示--一个向量序列，其中第i个向量对应于输入句子的第i个词。该架构类似于Transformer（Vaswani等人，2017），如图3的右边部分所示。然而，我们用我们提出的表格引导的注意力取代了按比例的点乘法注意力。在这里，我们主要说明为什么以及如何用**表格表示来计算注意力的权重**。

首先，给定Q（查询）、K（键）和V（值），图5中定义了一个广义的注意力形式。对于每个查询，输出是一个值的加权和，其中分配给每个值的权重是由查询与所有键的相关性（由评分函数f给出）决定的。对于每个查询Qi和键Kj，Bahdanau等人（2015）以下列形式定义f：



我们的注意机制本质上是一种自我注意机制，其中的查询、键和值是完全一样的。在我们的例子中，它们基本上是前一层的序列表示Sl-1（即Q = K = V = Sl-1）。注意力权重（即图5中函数f的输出）基本上是由查询和键构成的（在我们的例子中是相同的）。另一方面，我们也注意到表的表示方法 $T_l$ 也是由 $S_{l-1}$ 构建的。所以我们可以认为 $T_l$ 是一个查询和键的函数，这样$T_{l,i,j} = g(S_{l-1,i}, S_{l-1,j}) = g(Q_i, K_j)$。然后我们把这个$g$函数放回方程7中，得到提议的表引导的注意力，其得分函数为：

# Conclusion

在本文中，我们介绍了用于联合提取实体及其关系的新型 table-sequence encoders 架构。它学习两个独立的编码器，而不是一个：一个序列编码器和一个表格编码器，这两个编码器之间存在明确的相互作用。

我们还介绍了一种新的方法，以有效地利用预先训练好的语言模型所捕获的有用信息来完成这种涉及到表格表示的联合学习任务。我们在四个标准数据集的NER和RE任务中都取得了最先进的F1分数，这证实了我们方法的有效性。在未来，我们希望研究如何将表格表示应用于其他任务。另一个方向是将表格和序列的互动方式推广到其他类型的表示中。







